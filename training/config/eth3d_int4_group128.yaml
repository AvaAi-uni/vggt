# ============================================================================
# ETH3D + INT4 Group-wise Quantization (Group Size 128) 配置
# ============================================================================
#
# 实验目的：测试极致压缩的INT4量化方案
# 论文用途：展示INT4量化的可行性和挑战
#
# INT4 Group-128 量化特点：
# - 每128个权重共享一组量化参数
# - 4位量化：权重范围[-8, 7]或[-7, 7]
# - 优点：极高的压缩比，大幅降低存储和带宽需求
# - 缺点：精度损失较大，需要精心调优训练策略
#
# 理论压缩比：~7.8x（32bit → 4bit + group开销）
# 预期精度损失：5-15%（相比FP32）
#
# 论文研究价值：
# - 探索3D视觉任务的INT4量化极限
# - 分析Group Size对精度的影响
# - 提供INT4 QAT的最佳实践
#
# ============================================================================

defaults:
  - eth3d_fp32_baseline

# ============================================================================
# 实验标识
# ============================================================================
exp_name: eth3d_int4_group128
run_name: ${exp_name}

# ============================================================================
# 量化配置 - INT4 Group-wise (Group Size 128)
# ============================================================================
quantization:
  enabled: True
  current_scheme: "int4_group_128"

  # 量化感知训练(QAT)设置
  enable_qat: True
  qat_start_epoch: 0
  freeze_bn_in_qat: True

  # INT4 Group-wise量化参数
  bits: 4                    # 4位量化
  symmetric: True
  per_channel: False
  group_size: 128            # 每128个权重一组

  # 量化策略（INT4需要更保守的策略）
  skip_first_last: True      # 必须跳过第一层和最后一层
  quantize_activations: False # INT4只量化权重

  # 校准设置
  calibration_samples: 200   # INT4需要更多校准样本
  calibration_method: "percentile"  # 使用percentile方法减少异常值影响

# ============================================================================
# 训练参数（INT4需要更长的训练）
# ============================================================================
max_epochs: 40               # 比FP32多10个epoch

# Batch size 可能需要调小（INT4训练可能更不稳定）
max_img_per_gpu: 10

# ============================================================================
# 优化器配置（INT4需要特殊调优）
# ============================================================================
optim:
  param_group_modifiers: False

  optimizer:
    _target_: torch.optim.AdamW
    lr: 2e-5                 # 更小的学习率
    weight_decay: 0.1        # 更强的正则化
    betas: [0.9, 0.999]
    eps: 1e-8

  frozen_module_names: []

  amp:
    enabled: True
    amp_dtype: bfloat16

  gradient_clip:
    _target_: train_utils.gradient_clip.GradientClipper
    configs:
      - module_name: ["aggregator"]
        max_norm: 0.3        # 更严格的梯度裁剪
        norm_type: 2
      - module_name: ["depth"]
        max_norm: 0.3
        norm_type: 2
      - module_name: ["camera"]
        max_norm: 0.3
        norm_type: 2

  options:
    lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
            # 更长的warm-up
            - _target_: fvcore.common.param_scheduler.LinearParamScheduler
              start_value: 1e-8
              end_value: 2e-5
            - _target_: fvcore.common.param_scheduler.CosineParamScheduler
              start_value: 2e-5
              end_value: 1e-8
          lengths: [0.1, 0.9]  # 10% warm-up
          interval_scaling: ['rescaled', 'rescaled']

    weight_decay:
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.1

# ============================================================================
# 检查点配置
# ============================================================================
checkpoint:
  save_dir: ${logging.log_dir}/checkpoints
  save_freq: 5
  # 建议从INT8 Per-Channel开始fine-tune
  resume_checkpoint_path: logs/eth3d_int8_per_channel/checkpoints/checkpoint_best.pth
  strict: False

# ============================================================================
# 论文研究重点
# ============================================================================
# INT4量化的挑战与解决方案：
# 1. 量化误差分析：
#    - 不同层的量化误差分布
#    - 特别关注注意力层的量化表现
# 2. 训练策略：
#    - QAT vs PTQ效果对比
#    - Warm-up和学习率调度的重要性
# 3. Group Size影响：
#    - 与Group-64/32对比
#    - 精度-压缩比权衡分析
# 4. 实际应用价值：
#    - 在边缘设备上的部署可行性
#    - 推理速度提升
# ============================================================================
