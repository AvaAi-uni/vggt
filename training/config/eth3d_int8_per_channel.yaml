# ============================================================================
# ETH3D + INT8 Per-Channel Symmetric Quantization 配置
# ============================================================================
#
# 实验目的：测试精细化INT8量化方案性能
# 论文用途：展示Per-Channel量化相比Per-Tensor的优势
#
# INT8 Per-Channel Symmetric 量化特点：
# - 每个输出通道使用独立的量化参数
# - 对称量化：零点固定为0，只需学习缩放因子
# - 优点：能够更好地适应每个通道的权重分布，精度更高
# - 缺点：相比Per-Tensor略微增加存储和计算开销
#
# 理论压缩比：~3.8x（考虑每通道的缩放因子）
# 预期精度损失：<3%（优于Per-Tensor）
#
# 论文对比重点：
# - 与INT8 Per-Tensor对比，展示精度提升
# - 分析不同层的量化敏感性
# - 评估额外开销是否值得
#
# ============================================================================

defaults:
  - eth3d_fp32_baseline

# ============================================================================
# 实验标识
# ============================================================================
exp_name: eth3d_int8_per_channel
run_name: ${exp_name}

# ============================================================================
# 量化配置 - INT8 Per-Channel Symmetric
# ============================================================================
quantization:
  enabled: True
  current_scheme: "int8_per_channel_sym"

  # 量化感知训练(QAT)设置
  enable_qat: True
  qat_start_epoch: 0
  freeze_bn_in_qat: True

  # INT8 Per-Channel量化参数
  bits: 8
  symmetric: True
  per_channel: True          # 启用Per-Channel量化
  group_size: null           # Per-Channel不需要group

  # 量化策略
  skip_first_last: True      # 跳过第一层和最后一层
  quantize_activations: False # 只量化权重

  # 校准设置
  calibration_samples: 100
  calibration_method: "minmax"

# ============================================================================
# 优化器配置
# ============================================================================
optim:
  param_group_modifiers: False

  optimizer:
    _target_: torch.optim.AdamW
    lr: 3e-5                 # 与Per-Tensor相同
    weight_decay: 0.05
    betas: [0.9, 0.999]
    eps: 1e-8

  frozen_module_names: []

  amp:
    enabled: True
    amp_dtype: bfloat16

  gradient_clip:
    _target_: train_utils.gradient_clip.GradientClipper
    configs:
      - module_name: ["aggregator"]
        max_norm: 0.5
        norm_type: 2
      - module_name: ["depth"]
        max_norm: 0.5
        norm_type: 2
      - module_name: ["camera"]
        max_norm: 0.5
        norm_type: 2

  options:
    lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
            - _target_: fvcore.common.param_scheduler.LinearParamScheduler
              start_value: 1e-7
              end_value: 3e-5
            - _target_: fvcore.common.param_scheduler.CosineParamScheduler
              start_value: 3e-5
              end_value: 5e-8
          lengths: [0.05, 0.95]
          interval_scaling: ['rescaled', 'rescaled']

    weight_decay:
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.05

# ============================================================================
# 检查点配置
# ============================================================================
checkpoint:
  save_dir: ${logging.log_dir}/checkpoints
  save_freq: 5
  # 可以从FP32 baseline或INT8 Per-Tensor加载预训练权重
  resume_checkpoint_path: logs/eth3d_fp32_baseline/checkpoints/checkpoint_best.pth
  strict: False

# ============================================================================
# 论文对比重点
# ============================================================================
# Per-Channel vs Per-Tensor 对比分析：
# 1. 精度对比：
#    - 相机位姿误差提升百分比
#    - 深度图质量提升百分比
# 2. 效率对比：
#    - 模型大小增加（Per-Channel缩放因子开销）
#    - 推理时间差异
# 3. 层级分析：
#    - 哪些层从Per-Channel量化中受益最多？
#    - 注意力层 vs MLP层的量化敏感性
# ============================================================================
