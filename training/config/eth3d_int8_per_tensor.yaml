# ============================================================================
# ETH3D + INT8 Per-Tensor Symmetric Quantization 配置
# ============================================================================
#
# 实验目的：测试最简单的INT8量化方案性能
# 论文用途：展示基础量化对模型精度的影响
#
# INT8 Per-Tensor Symmetric 量化特点：
# - 每个张量（整个权重矩阵）使用一个量化参数
# - 对称量化：零点固定为0，只需学习缩放因子
# - 优点：实现简单，推理速度快，内存占用少
# - 缺点：对权重分布不均匀的层效果较差
#
# 理论压缩比：4x（32bit → 8bit）
# 预期精度损失：<5%
#
# ============================================================================

defaults:
  - eth3d_fp32_baseline   # 继承FP32 baseline的所有配置
  - _self_

# ============================================================================
# 实验标识
# ============================================================================
exp_name: eth3d_int8_per_tensor
run_name: ${exp_name}

# ============================================================================
# 量化配置 - INT8 Per-Tensor Symmetric
# ============================================================================
quantization:
  enabled: True
  current_scheme: "int8_per_tensor_sym"

  # 量化感知训练(QAT)设置
  enable_qat: True
  qat_start_epoch: 0         # 从第0个epoch开始QAT
  freeze_bn_in_qat: True     # QAT期间冻结BatchNorm

  # INT8 量化参数
  bits: 8                    # 8位量化
  symmetric: True            # 对称量化
  per_channel: False         # Per-Tensor（不是Per-Channel）
  group_size: null           # Per-Tensor不需要group

  # 量化策略
  skip_first_last: True      # 跳过第一层和最后一层（保持FP32）
  quantize_activations: False # 只量化权重，不量化激活值

  # 校准设置（用于Post-Training Quantization，QAT不需要）
  calibration_samples: 100
  calibration_method: "minmax"

# ============================================================================
# 优化器配置（相比FP32调整）
# ============================================================================
optim:
  param_group_modifiers: False

  # QAT通常使用较小的学习率
  optimizer:
    _target_: torch.optim.AdamW
    lr: 3e-5                 # 比FP32小（5e-5 → 3e-5）
    weight_decay: 0.05
    betas: [0.9, 0.999]
    eps: 1e-8

  frozen_module_names: []

  amp:
    enabled: True
    amp_dtype: bfloat16

  gradient_clip:
    _target_: train_utils.gradient_clip.GradientClipper
    configs:
      - module_name: ["aggregator"]
        max_norm: 0.5        # 更严格的梯度裁剪
        norm_type: 2
      - module_name: ["depth"]
        max_norm: 0.5
        norm_type: 2
      - module_name: ["camera"]
        max_norm: 0.5
        norm_type: 2

  # 学习率调度器
  options:
    lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
            - _target_: fvcore.common.param_scheduler.LinearParamScheduler
              start_value: 1e-7
              end_value: 3e-5
            - _target_: fvcore.common.param_scheduler.CosineParamScheduler
              start_value: 3e-5
              end_value: 5e-8
          lengths: [0.05, 0.95]
          interval_scaling: ['rescaled', 'rescaled']

    weight_decay:
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.05

# ============================================================================
# 检查点配置
# ============================================================================
checkpoint:
  save_dir: ${logging.log_dir}/checkpoints
  save_freq: 5
  # 可以从FP32 baseline加载预训练权重
  resume_checkpoint_path: logs/eth3d_fp32_baseline/checkpoints/checkpoint_best.pth
  strict: False

# ============================================================================
# 论文对比指标记录
# ============================================================================
# 训练完成后，请记录以下指标用于论文对比：
# 1. 模型大小（MB）
# 2. 推理时间（ms/image）
# 3. 相机位姿误差（Translation/Rotation Error）
# 4. 深度图误差（RMSE, MAE）
# 5. 训练时间（hours）
# 6. GPU内存占用（GB）
# ============================================================================
